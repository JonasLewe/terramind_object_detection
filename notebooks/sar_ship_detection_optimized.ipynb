{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasLewe/terramind_object_detection/blob/main/notebooks/sar_ship_detection_optimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "8Ppl5jsHO87Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "daH8iHWV60Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install terratorch==1.1.1 gdown tensorboard > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "8KodReoWxiGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import gdown\n",
        "import terratorch\n",
        "import albumentations\n",
        "import rasterio\n",
        "import rasterio.windows\n",
        "import rasterio.enums\n",
        "import json\n",
        "from collections import Counter\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightning.pytorch as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from pathlib import Path\n",
        "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "JJ24dhm6xmeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Tensorboard"
      ],
      "metadata": {
        "id": "EGz8B_Y8QjeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/xview3/runs"
      ],
      "metadata": {
        "id": "mJJhSbUuQivG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "LyZ7CDVviYNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/xview3/raw /content/xview3/chips /content/xview3/runs\n",
        "!mkdir -p /content/drive/MyDrive/xview3/{chips,runs,meta}"
      ],
      "metadata": {
        "id": "jLtq151L7JF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -ah --info=progress2 /content/drive/MyDrive/xview3/raw/ /content/xview3/raw/"
      ],
      "metadata": {
        "id": "kxCIXevg6sZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!for f in /content/xview3/raw/*.tar.gz; do tar -xzvf \"$f\" -C /content/xview3/raw && rm \"$f\"; done"
      ],
      "metadata": {
        "id": "UkB1e8j29uNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/xview3/meta\n",
        "!cp /content/drive/MyDrive/xview3/meta/train.csv /content/xview3/meta/train.csv\n",
        "!cp /content/drive/MyDrive/xview3/meta/validation.csv /content/xview3/meta/validation.csv\n",
        "!ls -lah /content/xview3/meta"
      ],
      "metadata": {
        "id": "WsrALZmEAG8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Exploration"
      ],
      "metadata": {
        "id": "mzoQDJVVxvLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = Path(\"/content/xview3\")\n",
        "RAW  = BASE / \"raw\"\n",
        "META = BASE / \"meta\"\n",
        "EXP  = BASE / \"explore\"\n",
        "CHIP = BASE / \"chips\"\n",
        "RUNS = BASE / \"runs\"\n",
        "\n",
        "# Memory-saving: Downsample factor for overview plots\n",
        "DOWNSAMPLE_FACTOR = 8"
      ],
      "metadata": {
        "id": "NiMLJgpK2nIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_scenes(raw_dir: Path):\n",
        "    \"\"\"Detect scene folders containing VV_dB.tif + VH_dB.tif\"\"\"\n",
        "    scenes = []\n",
        "    if not raw_dir.exists():\n",
        "        print(f\"Warning: {raw_dir} does not exist\")\n",
        "        return scenes\n",
        "\n",
        "    for d in sorted(raw_dir.iterdir()):\n",
        "        if d.is_dir() and (d / \"VV_dB.tif\").exists() and (d / \"VH_dB.tif\").exists():\n",
        "            scenes.append(d.name)\n",
        "    return scenes\n",
        "\n",
        "scenes = list_scenes(RAW)\n",
        "print(f\"Found {len(scenes)} scenes under {RAW}\")\n",
        "print(\"First scenes:\", scenes[:10])\n"
      ],
      "metadata": {
        "id": "hUJBFUcM0sIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Dataset Overview\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Load metadata\n",
        "train_df = pd.read_csv(META / \"train.csv\")\n",
        "val_df = pd.read_csv(META / \"validation.csv\")\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"xView3 Dataset Overview\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(f\"Training samples: {len(train_df):,}\")\n",
        "print(f\"Validation samples: {len(val_df):,}\")\n",
        "print(f\"Total labels: {len(train_df) + len(val_df):,}\\n\")\n",
        "\n",
        "# Scene distribution\n",
        "train_scenes = train_df['scene_id'].nunique()\n",
        "val_scenes = val_df['scene_id'].nunique()\n",
        "print(f\"Training scenes: {train_scenes}\")\n",
        "print(f\"Validation scenes: {val_scenes}\\n\")\n",
        "\n",
        "# Class distribution (assuming 'is_vessel', 'is_fishing' columns)\n",
        "if 'is_vessel' in train_df.columns:\n",
        "    print(\"Class Distribution (Train):\")\n",
        "    print(f\"  Vessels: {train_df['is_vessel'].sum():,}\")\n",
        "    # Fix: Compare to 0 instead of using ~\n",
        "    print(f\"  Non-vessels: {(train_df['is_vessel'] == 0).sum():,}\")\n",
        "    if 'is_fishing' in train_df.columns:\n",
        "        # Fix: Boolean indexing requires bool or comparison\n",
        "        fishing = train_df[train_df['is_vessel'] == 1]['is_fishing'].sum()\n",
        "        print(f\"  Fishing vessels: {fishing:,}\")\n",
        "        print(f\"  Non-fishing vessels: {train_df['is_vessel'].sum() - fishing:,}\\n\")\n",
        "\n",
        "# Labels per scene\n",
        "labels_per_scene = train_df.groupby('scene_id').size()\n",
        "print(f\"Labels per scene (train):\")\n",
        "print(f\"  Mean: {labels_per_scene.mean():.1f}\")\n",
        "print(f\"  Median: {labels_per_scene.median():.1f}\")\n",
        "print(f\"  Min/Max: {labels_per_scene.min()} / {labels_per_scene.max()}\")"
      ],
      "metadata": {
        "id": "UTrFIbqCCU4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Helper Functions"
      ],
      "metadata": {
        "id": "irb8wOakHTAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sar_scene(scene_dir: Path, bands=[\"VV\", \"VH\"],\n",
        "                   downsample=1, window=None, nodata_threshold=-9999):\n",
        "    \"\"\"\n",
        "    Load VV and VH bands from scene directory.\n",
        "\n",
        "    Args:\n",
        "        scene_dir: Path to scene folder\n",
        "        bands: List of bands to load\n",
        "        downsample: Factor for downsampling (e.g., 8 = 1/8 resolution)\n",
        "        window: rasterio.windows.Window for chip extraction (overrides downsample)\n",
        "        nodata_threshold: Values <= this are treated as NoData (replaced with NaN)\n",
        "\n",
        "    Returns:\n",
        "        dict with band data and metadata\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    for band in bands:\n",
        "        path = scene_dir / f\"{band}_dB.tif\"\n",
        "        with rasterio.open(path) as src:\n",
        "            if window is not None:\n",
        "                arr = src.read(1, window=window).astype(\"float32\")\n",
        "                if band == bands[0]:\n",
        "                    data['meta'] = {\n",
        "                        'transform': src.window_transform(window),\n",
        "                        'crs': src.crs,\n",
        "                        'bounds': src.bounds,\n",
        "                        'shape': src.shape,\n",
        "                        'loaded_shape': arr.shape\n",
        "                    }\n",
        "            elif downsample > 1:\n",
        "                h, w = src.height // downsample, src.width // downsample\n",
        "                arr = src.read(\n",
        "                    1,\n",
        "                    out_shape=(h, w),\n",
        "                    resampling=rasterio.enums.Resampling.average\n",
        "                ).astype(\"float32\")\n",
        "                if band == bands[0]:\n",
        "                    data['meta'] = {\n",
        "                        'transform': src.transform * src.transform.scale(\n",
        "                            src.width / w,\n",
        "                            src.height / h\n",
        "                        ),\n",
        "                        'crs': src.crs,\n",
        "                        'bounds': src.bounds,\n",
        "                        'shape': src.shape,\n",
        "                        'loaded_shape': (h, w),\n",
        "                        'downsample': downsample\n",
        "                    }\n",
        "            else:\n",
        "                arr = src.read(1).astype(\"float32\")\n",
        "                if band == bands[0]:\n",
        "                    data['meta'] = {\n",
        "                        'transform': src.transform,\n",
        "                        'crs': src.crs,\n",
        "                        'bounds': src.bounds,\n",
        "                        'shape': src.shape,\n",
        "                        'loaded_shape': src.shape\n",
        "                    }\n",
        "\n",
        "            # Replace NoData with NaN\n",
        "            arr = np.where(arr <= nodata_threshold, np.nan, arr)\n",
        "            data[band] = arr\n",
        "\n",
        "    return data\n",
        "\n",
        "def sar_to_rgb(vv, vh, percentile_clip=(2, 98)):\n",
        "    \"\"\"\n",
        "    Create RGB composite from SAR dual-pol data\n",
        "    R = VV, G = VH, B = VV/VH ratio (cross-pol)\n",
        "    \"\"\"\n",
        "    # Clip outliers\n",
        "    vv_clip = np.nanpercentile(vv, percentile_clip)\n",
        "    vh_clip = np.nanpercentile(vh, percentile_clip)\n",
        "\n",
        "    vv_norm = np.clip((vv - vv_clip[0]) / (vv_clip[1] - vv_clip[0] + 1e-6), 0, 1)\n",
        "    vh_norm = np.clip((vh - vh_clip[0]) / (vh_clip[1] - vh_clip[0] + 1e-6), 0, 1)\n",
        "\n",
        "    # Cross-pol ratio (indicator for surface roughness)\n",
        "    ratio = np.where(vh != 0, vv / (vh + 1e-6), 0)\n",
        "    ratio_clip = np.nanpercentile(ratio[np.isfinite(ratio)], percentile_clip)\n",
        "    ratio_norm = np.clip((ratio - ratio_clip[0]) / (ratio_clip[1] - ratio_clip[0] + 1e-6), 0, 1)\n",
        "\n",
        "    rgb = np.stack([vv_norm, vh_norm, ratio_norm], axis=-1)\n",
        "    return rgb\n",
        "\n",
        "def latlon_to_pixel(lats, lons, transform, crs, downsample=1):\n",
        "    \"\"\"\n",
        "    Convert lat/lon to pixel coordinates.\n",
        "\n",
        "    Args:\n",
        "        downsample: If image was downsampled, adjust pixel coords accordingly\n",
        "    \"\"\"\n",
        "    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", crs, always_xy=True)\n",
        "    xs, ys = transformer.transform(lons, lats)\n",
        "\n",
        "    # Rasterio transform: pixel = ~transform * (x, y)\n",
        "    inv_transform = ~transform\n",
        "    pixels = [inv_transform * (x, y) for x, y in zip(xs, ys)]\n",
        "    cols = np.array([p[0] for p in pixels], dtype=int)\n",
        "    rows = np.array([p[1] for p in pixels], dtype=int)\n",
        "\n",
        "    # Adjust for downsampling\n",
        "    if downsample > 1:\n",
        "        cols = cols // downsample\n",
        "        rows = rows // downsample\n",
        "\n",
        "    return rows, cols\n",
        "\n",
        "def plot_sar_with_labels(scene_id, df, raw_dir=RAW, max_labels=None, downsample=DOWNSAMPLE_FACTOR):\n",
        "    \"\"\"\n",
        "    Plot SAR scene with overlaid labels (memory-efficient with downsampling)\n",
        "    \"\"\"\n",
        "    scene_dir = raw_dir / scene_id\n",
        "    scene_labels = df[df['scene_id'] == scene_id].copy()\n",
        "\n",
        "    if max_labels:\n",
        "        scene_labels = scene_labels.head(max_labels)\n",
        "\n",
        "    # Load SAR data with downsampling for memory efficiency\n",
        "    sar_data = load_sar_scene(scene_dir, downsample=downsample)\n",
        "    vv, vh = sar_data['VV'], sar_data['VH']\n",
        "    meta = sar_data['meta']\n",
        "\n",
        "    # Convert labels to pixels (using original transform, then scale)\n",
        "    with rasterio.open(scene_dir / \"VV_dB.tif\") as src:\n",
        "        orig_transform = src.transform\n",
        "        orig_crs = src.crs\n",
        "\n",
        "    rows, cols = latlon_to_pixel(\n",
        "        scene_labels['detect_lat'].values,\n",
        "        scene_labels['detect_lon'].values,\n",
        "        orig_transform,\n",
        "        orig_crs,\n",
        "        downsample=downsample\n",
        "    )\n",
        "\n",
        "    # Create RGB composite\n",
        "    rgb = sar_to_rgb(vv, vh)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # VV band\n",
        "    axes[0].imshow(vv, cmap='gray', vmin=np.nanpercentile(vv, 2), vmax=np.nanpercentile(vv, 98))\n",
        "    axes[0].scatter(cols, rows, c='red', s=30, marker='x', alpha=0.7, linewidths=2)\n",
        "    axes[0].set_title(f'VV (dB) - {len(scene_labels)} labels')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # VH band\n",
        "    axes[1].imshow(vh, cmap='gray', vmin=np.nanpercentile(vh, 2), vmax=np.nanpercentile(vh, 98))\n",
        "    axes[1].scatter(cols, rows, c='red', s=30, marker='x', alpha=0.7, linewidths=2)\n",
        "    axes[1].set_title('VH (dB)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # RGB Composite\n",
        "    axes[2].imshow(rgb)\n",
        "    axes[2].scatter(cols, rows, c='cyan', s=30, marker='x', alpha=0.8, linewidths=2)\n",
        "    axes[2].set_title('RGB Composite (VV/VH/Ratio)')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    ds_info = f\" (1/{downsample} res)\" if downsample > 1 else \"\"\n",
        "    plt.suptitle(f'Scene: {scene_id} | Original: {meta[\"shape\"]}{ds_info}', fontsize=14, y=0.98)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print stats\n",
        "    print(f\"\\nScene Stats:\")\n",
        "    print(f\"  VV range: [{np.nanmin(vv):.2f}, {np.nanmax(vv):.2f}] dB\")\n",
        "    print(f\"  VH range: [{np.nanmin(vh):.2f}, {np.nanmax(vh):.2f}] dB\")\n",
        "    print(f\"  Labels in scene: {len(scene_labels)}\")\n",
        "    print(f\"  Loaded shape: {meta['loaded_shape']}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del sar_data, vv, vh, rgb\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "hAmquOapGgVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Detailed Scene Exploration\n",
        "\n",
        "# Pick a scene with reasonable number of labels (automatic selection)\n",
        "scene_counts = train_df['scene_id'].value_counts()\n",
        "\n",
        "# Filter to scenes that actually exist in RAW folder\n",
        "available_scenes = set(scenes)\n",
        "valid_scene_counts = scene_counts[scene_counts.index.isin(available_scenes)]\n",
        "\n",
        "# Select scene with 10-100 labels\n",
        "candidates = valid_scene_counts[valid_scene_counts.between(10, 100)]\n",
        "if len(candidates) > 0:\n",
        "    selected_scene = candidates.index[0]\n",
        "else:\n",
        "    # Fallback to any available scene\n",
        "    selected_scene = valid_scene_counts.index[0]\n",
        "\n",
        "print(f\"Analyzing scene: {selected_scene} ({valid_scene_counts[selected_scene]} labels)\")\n",
        "plot_sar_with_labels(selected_scene, train_df, max_labels=50)"
      ],
      "metadata": {
        "id": "Kftggje0HOev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Multi-Scene Gallery\n",
        "\n",
        "# Filter scene_counts to only available scenes\n",
        "available_scenes = set(scenes)\n",
        "valid_scene_counts = scene_counts[scene_counts.index.isin(available_scenes)]\n",
        "\n",
        "# Select diverse scenes (different label counts)\n",
        "low_count = valid_scene_counts[valid_scene_counts < 10].index[:2]\n",
        "mid_count = valid_scene_counts[valid_scene_counts.between(10, 50)].index[:2]\n",
        "high_count = valid_scene_counts[valid_scene_counts > 50].index[:2]\n",
        "\n",
        "gallery_scenes = list(low_count) + list(mid_count) + list(high_count)\n",
        "\n",
        "for scene_id in gallery_scenes[:6]:  # Show first 6\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    plot_sar_with_labels(scene_id, train_df, max_labels=30)"
      ],
      "metadata": {
        "id": "hfaRezZfHda6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Zoomed-in Label Chips (Memory-efficient with windowed reads)\n",
        "\n",
        "def plot_label_chips(scene_id, df, n_chips=9, chip_size=128):\n",
        "    \"\"\"Plot grid of zoomed-in chips around labels using windowed reads\"\"\"\n",
        "    scene_labels_full = df[df['scene_id'] == scene_id]\n",
        "    scene_labels = scene_labels_full.sample(min(n_chips, len(scene_labels_full)))\n",
        "\n",
        "    scene_dir = RAW / scene_id\n",
        "    vv_path = scene_dir / \"VV_dB.tif\"\n",
        "    vh_path = scene_dir / \"VH_dB.tif\"\n",
        "\n",
        "    # Open files once, read chips as needed\n",
        "    with rasterio.open(vv_path) as vv_src, rasterio.open(vh_path) as vh_src:\n",
        "        transform = vv_src.transform\n",
        "        crs = vv_src.crs\n",
        "        img_height, img_width = vv_src.height, vv_src.width\n",
        "\n",
        "        # Convert labels to pixels\n",
        "        rows, cols = latlon_to_pixel(\n",
        "            scene_labels['detect_lat'].values,\n",
        "            scene_labels['detect_lon'].values,\n",
        "            transform,\n",
        "            crs\n",
        "        )\n",
        "\n",
        "        n_cols_plot = 3\n",
        "        n_rows_plot = (len(scene_labels) + n_cols_plot - 1) // n_cols_plot\n",
        "        fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(12, 4*n_rows_plot))\n",
        "        axes = np.array(axes).flatten() if n_rows_plot > 1 else [axes] if n_rows_plot == 1 else axes\n",
        "\n",
        "        for idx, (row, col) in enumerate(zip(rows, cols)):\n",
        "            # Calculate window bounds with boundary checks\n",
        "            half = chip_size // 2\n",
        "            c_start = max(0, col - half)\n",
        "            r_start = max(0, row - half)\n",
        "            c_end = min(img_width, col + half)\n",
        "            r_end = min(img_height, row + half)\n",
        "\n",
        "            window = rasterio.windows.Window(\n",
        "                col_off=c_start,\n",
        "                row_off=r_start,\n",
        "                width=c_end - c_start,\n",
        "                height=r_end - r_start\n",
        "            )\n",
        "\n",
        "            # Read only the chip (memory efficient!)\n",
        "            vv_chip = vv_src.read(1, window=window).astype(\"float32\")\n",
        "            vh_chip = vh_src.read(1, window=window).astype(\"float32\")\n",
        "            rgb_chip = sar_to_rgb(vv_chip, vh_chip)\n",
        "\n",
        "            # Center marker (relative to chip)\n",
        "            center_col = col - c_start\n",
        "            center_row = row - r_start\n",
        "\n",
        "            axes[idx].imshow(rgb_chip)\n",
        "            axes[idx].scatter([center_col], [center_row], c='red', s=100, marker='+', linewidths=3)\n",
        "            axes[idx].set_title(f'Label {idx+1}', fontsize=10)\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        # Hide empty subplots\n",
        "        for idx in range(len(scene_labels), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle(f'Label Chips from {scene_id}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "# Run for selected scene\n",
        "plot_label_chips(selected_scene, train_df, n_chips=9, chip_size=192)"
      ],
      "metadata": {
        "id": "jMGsgEjoIgc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Data Quality Analysis (Memory-efficient sampling)\n",
        "\n",
        "def analyze_data_quality(df, scene_sample=10):\n",
        "    \"\"\"Check for data quality issues using memory-efficient sampling\"\"\"\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"Data Quality Checks\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # 1. Missing values\n",
        "    print(\"Missing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "    print()\n",
        "\n",
        "    # 2. Coordinate ranges\n",
        "    print(\"Coordinate Ranges:\")\n",
        "    print(f\"  Latitude: [{df['detect_lat'].min():.4f}, {df['detect_lat'].max():.4f}]\")\n",
        "    print(f\"  Longitude: [{df['detect_lon'].min():.4f}, {df['detect_lon'].max():.4f}]\")\n",
        "    print()\n",
        "\n",
        "    # 3. Sample scenes for image quality (memory efficient)\n",
        "    available_scenes = set(scenes)\n",
        "    sample_scene_ids = df['scene_id'].value_counts().head(scene_sample * 2).index\n",
        "    sample_scene_ids = [s for s in sample_scene_ids if s in available_scenes][:scene_sample]\n",
        "\n",
        "    vv_ranges, vh_ranges = [], []\n",
        "    nan_counts = []\n",
        "\n",
        "    print(f\"Sampling {len(sample_scene_ids)} scenes for statistics...\")\n",
        "\n",
        "    for scene_id in sample_scene_ids:\n",
        "        scene_dir = RAW / scene_id\n",
        "        if not scene_dir.exists():\n",
        "            continue\n",
        "\n",
        "        # Memory-efficient: read downsampled version\n",
        "        sar_data = load_sar_scene(scene_dir, downsample=16)  # Very small for stats\n",
        "        vv, vh = sar_data['VV'], sar_data['VH']\n",
        "\n",
        "        vv_ranges.append((np.nanmin(vv), np.nanmax(vv)))\n",
        "        vh_ranges.append((np.nanmin(vh), np.nanmax(vh)))\n",
        "        nan_counts.append((np.isnan(vv).sum(), np.isnan(vh).sum()))\n",
        "\n",
        "        del sar_data, vv, vh\n",
        "        gc.collect()\n",
        "\n",
        "    if vv_ranges:\n",
        "        print(f\"\\nSAR Value Ranges (sampled {len(vv_ranges)} scenes):\")\n",
        "        vv_mins, vv_maxs = zip(*vv_ranges)\n",
        "        vh_mins, vh_maxs = zip(*vh_ranges)\n",
        "        print(f\"  VV: [{np.mean(vv_mins):.2f}, {np.mean(vv_maxs):.2f}] dB (avg)\")\n",
        "        print(f\"  VH: [{np.mean(vh_mins):.2f}, {np.mean(vh_maxs):.2f}] dB (avg)\")\n",
        "        print(f\"  NaN pixels (VV): {np.mean([n[0] for n in nan_counts]):.0f} avg (in downsampled)\")\n",
        "        print(f\"  NaN pixels (VH): {np.mean([n[1] for n in nan_counts]):.0f} avg (in downsampled)\")\n",
        "\n",
        "analyze_data_quality(train_df, scene_sample=10)"
      ],
      "metadata": {
        "id": "PUbyZuDTIi7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation Functions"
      ],
      "metadata": {
        "id": "ZANgFceCn0qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DATASET GENERATION CONFIG\n",
        "# ============================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Chip settings\n",
        "    \"chip_size\": 256,              # Kleiner für Platzersparnis\n",
        "    \"save_dtype\": np.int16,        # Speicher-Format\n",
        "    \"save_scale\": 10,             # dB * 100 → Int16 (behält 2 Dezimalstellen)\n",
        "    \"compression\": \"lzw\",\n",
        "    \"pixel_size_m\": 10,           # Sentinel-1 ~10m resolution\n",
        "\n",
        "    # Mask generation\n",
        "    \"mask_type\": \"circle\",        # Einfach, klar definiert\n",
        "    \"mask_radius_default_m\": 150, # Sichtbar, aber nicht riesig\n",
        "    \"mask_radius_min_px\": 8,\n",
        "    \"mask_radius_max_px\": 30,\n",
        "    \"gaussian_sigma_factor\": 0.5,  # sigma = radius * factor\n",
        "\n",
        "    # Sampling strategy\n",
        "    \"positive_chips_per_ship\": 1,  # Chips zentriert auf Schiffe\n",
        "    \"negative_ratio\": 0.3,         # 30% Negative Chips\n",
        "    \"negative_min_water_frac\": 0.7,# Negatives: mind. 70% Wasser\n",
        "    \"random_offset_px\": 30,       # Jitter für positive chips\n",
        "\n",
        "    # Data filtering\n",
        "    # \"min_confidence\": 0.5,         # xView3 confidence threshold\n",
        "    \"only_confirmed_vessels\": True,# Nur is_vessel == 1\n",
        "\n",
        "    # Output\n",
        "    \"output_dir\": Path(\"/content/drive/MyDrive/xview3/chips\"),\n",
        "    \"n_workers\": 4,\n",
        "    \"max_scenes\": 50,\n",
        "\n",
        "    # Split (train.csv wird zu train+test, validation.csv bleibt val)\n",
        "    \"test_fraction\": 0.1,          # 10% von train.csv → test\n",
        "}\n",
        "\n",
        "# Derived\n",
        "CONFIG[\"mask_radius_default_px\"] = CONFIG[\"mask_radius_default_m\"] // CONFIG[\"pixel_size_m\"]\n",
        "\n",
        "print(f\"Default mask radius: {CONFIG['mask_radius_default_px']} px\")"
      ],
      "metadata": {
        "id": "ZkYFB09FjcDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.draw import disk\n",
        "import rasterio.windows\n",
        "\n",
        "def get_ship_radius_px(vessel_length_m, config=CONFIG):\n",
        "    \"\"\"Convert vessel length to pixel radius\"\"\"\n",
        "    if pd.isna(vessel_length_m) or vessel_length_m <= 0:\n",
        "        return config[\"mask_radius_default_px\"]\n",
        "\n",
        "    # Radius = halbe Länge, konvertiert zu Pixeln\n",
        "    radius = (vessel_length_m / 2) / config[\"pixel_size_m\"]\n",
        "    return int(np.clip(radius, config[\"mask_radius_min_px\"], config[\"mask_radius_max_px\"]))\n",
        "\n",
        "\n",
        "def create_mask(shape, points, radii, config=CONFIG):\n",
        "    \"\"\"\n",
        "    Create segmentation mask from point labels.\n",
        "\n",
        "    Args:\n",
        "        shape: (H, W) of output mask\n",
        "        points: List of (row, col) tuples\n",
        "        radii: List of radii in pixels\n",
        "    \"\"\"\n",
        "    mask = np.zeros(shape, dtype=np.float32)\n",
        "\n",
        "    for (row, col), radius in zip(points, radii):\n",
        "        if config[\"mask_type\"] == \"circle\":\n",
        "            rr, cc = disk((row, col), radius, shape=shape)\n",
        "            mask[rr, cc] = 1.0\n",
        "        else:  # gaussian\n",
        "            # Create small gaussian, paste into mask\n",
        "            size = radius * 4\n",
        "            y, x = np.ogrid[-size:size+1, -size:size+1]\n",
        "            sigma = radius * config[\"gaussian_sigma_factor\"]\n",
        "            gaussian = np.exp(-(x*x + y*y) / (2*sigma*sigma))\n",
        "\n",
        "            # Bounds check\n",
        "            r_start = max(0, row - size)\n",
        "            r_end = min(shape[0], row + size + 1)\n",
        "            c_start = max(0, col - size)\n",
        "            c_end = min(shape[1], col + size + 1)\n",
        "\n",
        "            g_r_start = size - (row - r_start)\n",
        "            g_r_end = size + (r_end - row)\n",
        "            g_c_start = size - (col - c_start)\n",
        "            g_c_end = size + (c_end - col)\n",
        "\n",
        "            mask[r_start:r_end, c_start:c_end] = np.maximum(\n",
        "                mask[r_start:r_end, c_start:c_end],\n",
        "                gaussian[g_r_start:g_r_end, g_c_start:g_c_end]\n",
        "            )\n",
        "\n",
        "    return (mask > 0.5).astype(np.uint8) if config[\"mask_type\"] == \"circle\" else mask\n",
        "\n",
        "\n",
        "def estimate_water_fraction(vv_chip, water_threshold=-15):\n",
        "    \"\"\"Estimate fraction of water pixels (low backscatter)\"\"\"\n",
        "    valid = ~np.isnan(vv_chip)\n",
        "    if valid.sum() == 0:\n",
        "        return 0\n",
        "    water = (vv_chip < water_threshold) & valid\n",
        "    return water.sum() / valid.sum()\n",
        "\n",
        "\n",
        "def extract_chip_with_window(src_vv, src_vh, row, col, size):\n",
        "    \"\"\"Extract chip using rasterio window (memory efficient)\"\"\"\n",
        "    window = rasterio.windows.Window(\n",
        "        col_off=col, row_off=row, width=size, height=size\n",
        "    )\n",
        "\n",
        "    # Bounds check\n",
        "    if (row < 0 or col < 0 or\n",
        "        row + size > src_vv.height or\n",
        "        col + size > src_vv.width):\n",
        "        return None, None\n",
        "\n",
        "    vv = src_vv.read(1, window=window).astype(np.float32)\n",
        "    vh = src_vh.read(1, window=window).astype(np.float32)\n",
        "\n",
        "    # Replace nodata\n",
        "    vv = np.where(vv <= -9999, np.nan, vv)\n",
        "    vh = np.where(vh <= -9999, np.nan, vh)\n",
        "\n",
        "    # Skip if too much nodata\n",
        "    if np.isnan(vv).sum() > (size * size * 0.3):\n",
        "        return None, None\n",
        "\n",
        "    return vv, vh"
      ],
      "metadata": {
        "id": "eH_Gv_TmoEBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SCENE PROCESSOR\n",
        "# ============================================================\n",
        "\n",
        "def process_scene(scene_id, labels_df, config=CONFIG):\n",
        "    \"\"\"\n",
        "    Process one scene: extract positive and negative chips.\n",
        "\n",
        "    Returns: List of (chip_id, vv, vh, mask, is_positive)\n",
        "    \"\"\"\n",
        "    scene_dir = RAW / scene_id\n",
        "    scene_labels = labels_df[labels_df[\"scene_id\"] == scene_id].copy()\n",
        "\n",
        "    chips = []\n",
        "    size = config[\"chip_size\"]\n",
        "\n",
        "    with rasterio.open(scene_dir / \"VV_dB.tif\") as src_vv, \\\n",
        "         rasterio.open(scene_dir / \"VH_dB.tif\") as src_vh:\n",
        "\n",
        "        transform = src_vv.transform\n",
        "        crs = src_vv.crs\n",
        "        img_h, img_w = src_vv.height, src_vv.width\n",
        "\n",
        "        # Convert all labels to pixel coords\n",
        "        if len(scene_labels) > 0:\n",
        "            rows, cols = latlon_to_pixel(\n",
        "                scene_labels[\"detect_lat\"].values,\n",
        "                scene_labels[\"detect_lon\"].values,\n",
        "                transform, crs\n",
        "            )\n",
        "            scene_labels[\"pixel_row\"] = rows\n",
        "            scene_labels[\"pixel_col\"] = cols\n",
        "\n",
        "            # Filter labels outside image bounds\n",
        "            valid = (rows >= 0) & (rows < img_h) & (cols >= 0) & (cols < img_w)\n",
        "            scene_labels = scene_labels[valid].copy()\n",
        "\n",
        "        # ========== POSITIVE CHIPS ==========\n",
        "        for idx, label in scene_labels.iterrows():\n",
        "            row, col = int(label[\"pixel_row\"]), int(label[\"pixel_col\"])\n",
        "\n",
        "            # Add random offset\n",
        "            offset = config[\"random_offset_px\"]\n",
        "            row_start = row - size // 2 + np.random.randint(-offset, offset + 1)\n",
        "            col_start = col - size // 2 + np.random.randint(-offset, offset + 1)\n",
        "\n",
        "            # Clamp to image bounds\n",
        "            row_start = max(0, min(img_h - size, row_start))\n",
        "            col_start = max(0, min(img_w - size, col_start))\n",
        "\n",
        "            vv, vh = extract_chip_with_window(src_vv, src_vh, row_start, col_start, size)\n",
        "            if vv is None:\n",
        "                continue\n",
        "\n",
        "            # Find ALL labels in this chip\n",
        "            in_chip = scene_labels[\n",
        "                (scene_labels[\"pixel_row\"] >= row_start) &\n",
        "                (scene_labels[\"pixel_row\"] < row_start + size) &\n",
        "                (scene_labels[\"pixel_col\"] >= col_start) &\n",
        "                (scene_labels[\"pixel_col\"] < col_start + size)\n",
        "            ]\n",
        "\n",
        "            if len(in_chip) == 0:\n",
        "                continue\n",
        "\n",
        "            # Create mask\n",
        "            points = [\n",
        "                (int(r - row_start), int(c - col_start))\n",
        "                for r, c in zip(in_chip[\"pixel_row\"], in_chip[\"pixel_col\"])\n",
        "            ]\n",
        "            radii = [get_ship_radius_px(l) for l in in_chip[\"vessel_length_m\"]]\n",
        "            mask = create_mask((size, size), points, radii, config)\n",
        "\n",
        "            chip_id = f\"{scene_id}_{row_start}_{col_start}\"\n",
        "            chips.append((chip_id, vv, vh, mask, True))\n",
        "\n",
        "        # ========== NEGATIVE CHIPS ==========\n",
        "        n_positive = len([c for c in chips if c[4]])\n",
        "        n_negative_target = int(n_positive * config[\"negative_ratio\"] / (1 - config[\"negative_ratio\"]))\n",
        "\n",
        "        attempts = 0\n",
        "        negatives_found = 0\n",
        "\n",
        "        while negatives_found < n_negative_target and attempts < n_negative_target * 10:\n",
        "            attempts += 1\n",
        "\n",
        "            row_start = np.random.randint(0, max(1, img_h - size))\n",
        "            col_start = np.random.randint(0, max(1, img_w - size))\n",
        "\n",
        "            # Check no ships in this area\n",
        "            in_chip = scene_labels[\n",
        "                (scene_labels[\"pixel_row\"] >= row_start - 50) &\n",
        "                (scene_labels[\"pixel_row\"] < row_start + size + 50) &\n",
        "                (scene_labels[\"pixel_col\"] >= col_start - 50) &\n",
        "                (scene_labels[\"pixel_col\"] < col_start + size + 50)\n",
        "            ]\n",
        "\n",
        "            if len(in_chip) > 0:\n",
        "                continue\n",
        "\n",
        "            vv, vh = extract_chip_with_window(src_vv, src_vh, row_start, col_start, size)\n",
        "            if vv is None:\n",
        "                continue\n",
        "\n",
        "            # Prefer water chips\n",
        "            water_frac = estimate_water_fraction(vv)\n",
        "            if water_frac < config[\"negative_min_water_frac\"]:\n",
        "                if np.random.random() > 0.2:  # 20% chance to keep non-water\n",
        "                    continue\n",
        "\n",
        "            mask = np.zeros((size, size), dtype=np.float32)\n",
        "            chip_id = f\"{scene_id}_{row_start}_{col_start}_neg\"\n",
        "            chips.append((chip_id, vv, vh, mask, False))\n",
        "            negatives_found += 1\n",
        "\n",
        "    return chips"
      ],
      "metadata": {
        "id": "dOM5616toIj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TEST RUN - 1 Scene, 5 Chips, Local Storage\n",
        "# ============================================================\n",
        "\n",
        "def test_pipeline(n_chips=5):\n",
        "    \"\"\"Quick test before full run\"\"\"\n",
        "\n",
        "    # Temp config\n",
        "    test_config = CONFIG.copy()\n",
        "    test_config[\"output_dir\"] = Path(\"/content/xview3/test_chips\")\n",
        "    test_config[\"positive_chips_per_ship\"] = 1\n",
        "    test_config[\"negative_ratio\"] = 0.3\n",
        "\n",
        "    # Pick 1 scene with decent labels\n",
        "    test_scene = scenes[0]\n",
        "    test_labels = train_df[train_df[\"scene_id\"] == test_scene].head(n_chips)\n",
        "\n",
        "    print(f\"Testing with scene: {test_scene}\")\n",
        "    print(f\"Labels in test: {len(test_labels)}\")\n",
        "\n",
        "    # Process\n",
        "    try:\n",
        "        chips = process_scene(test_scene, train_df, test_config)\n",
        "        chips = chips[:n_chips]\n",
        "        print(f\"✓ process_scene: {len(chips)} chips generated\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ process_scene FAILED: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    # Save\n",
        "    try:\n",
        "        for chip_id, vv, vh, mask, is_pos in chips:\n",
        "            save_chip(chip_id, vv, vh, mask, test_config[\"output_dir\"], test_config)\n",
        "        print(f\"✓ save_chip: OK\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ save_chip FAILED: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    # Verify saved files\n",
        "    img_dir = test_config[\"output_dir\"] / \"images\"\n",
        "    mask_dir = test_config[\"output_dir\"] / \"masks\"\n",
        "\n",
        "    saved_imgs = list(img_dir.glob(\"*.tif\"))\n",
        "    saved_masks = list(mask_dir.glob(\"*.tif\"))\n",
        "    print(f\"✓ Saved: {len(saved_imgs)} images, {len(saved_masks)} masks\")\n",
        "\n",
        "    # Load back & visualize\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"VISUAL VERIFICATION\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    n_show = min(3, len(saved_imgs))\n",
        "    fig, axes = plt.subplots(n_show, 3, figsize=(12, 4*n_show))\n",
        "    if n_show == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, img_path in enumerate(saved_imgs[:n_show]):\n",
        "        mask_path = mask_dir / img_path.name\n",
        "\n",
        "        with rasterio.open(img_path) as src:\n",
        "            vv = src.read(1).astype(np.float32) / test_config[\"save_scale\"]\n",
        "            vh = src.read(2).astype(np.float32) / test_config[\"save_scale\"]\n",
        "\n",
        "        with rasterio.open(mask_path) as src:\n",
        "            mask = src.read(1)\n",
        "\n",
        "        rgb = sar_to_rgb(vv, vh)\n",
        "\n",
        "        axes[idx, 0].imshow(rgb)\n",
        "        axes[idx, 0].set_title(f\"RGB ({img_path.stem[:25]}...)\")\n",
        "        axes[idx, 0].axis(\"off\")\n",
        "\n",
        "        axes[idx, 1].imshow(mask, cmap=\"Reds\", vmin=0, vmax=1)\n",
        "        axes[idx, 1].set_title(f\"Mask ({mask.sum()} px)\")\n",
        "        axes[idx, 1].axis(\"off\")\n",
        "\n",
        "        axes[idx, 2].imshow(rgb)\n",
        "        axes[idx, 2].imshow(mask, cmap=\"Reds\", alpha=0.5)\n",
        "        axes[idx, 2].set_title(\"Overlay\")\n",
        "        axes[idx, 2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================================\n",
        "    # SIZE ESTIMATION - Korrigiert für tatsächlich verfügbare Daten\n",
        "    # ============================================================\n",
        "    total_size = sum(f.stat().st_size for f in saved_imgs + saved_masks)\n",
        "    avg_size_kb = total_size / len(saved_imgs) / 1024\n",
        "\n",
        "    # Filter labels to scenes die tatsächlich existieren\n",
        "    train_labels_available = train_df[train_df[\"scene_id\"].isin(scenes)]\n",
        "    val_labels_available = val_df[val_df[\"scene_id\"].isin(scenes)]\n",
        "    total_labels = len(train_labels_available) + len(val_labels_available)\n",
        "\n",
        "    # Schätze Chips: Labels + 30% negatives\n",
        "    estimated_chips = int(total_labels * 1.3)\n",
        "    estimated_gb = avg_size_kb * estimated_chips / 1024 / 1024\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SIZE ESTIMATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Available scenes: {len(scenes)}\")\n",
        "    print(f\"Labels in available scenes: {total_labels:,}\")\n",
        "    print(f\"Avg chip size: {avg_size_kb:.1f} KB\")\n",
        "    print(f\"Estimated chips: ~{estimated_chips:,}\")\n",
        "    print(f\"Estimated dataset size: {estimated_gb:.2f} GB\")\n",
        "\n",
        "    print(f\"\\n✅ Test passed! Run full generation when ready.\")\n",
        "    return True\n",
        "\n",
        "# RUN TEST\n",
        "test_pipeline(n_chips=5)"
      ],
      "metadata": {
        "id": "aoLHt1sgp1Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute Dataset Generation"
      ],
      "metadata": {
        "id": "xPFJTiAup_kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MAIN PROCESSING LOOP\n",
        "# ============================================================\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import traceback\n",
        "\n",
        "def save_chip(chip_id, vv, vh, mask, output_dir, config=CONFIG):\n",
        "    \"\"\"Save chip as 2-band image + mask (compressed)\"\"\"\n",
        "    img_dir = output_dir / \"images\"\n",
        "    mask_dir = output_dir / \"masks\"\n",
        "    img_dir.mkdir(parents=True, exist_ok=True)\n",
        "    mask_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Stack VV + VH\n",
        "    img_stack = np.stack([vv, vh], axis=0)\n",
        "\n",
        "    # Convert for saving: float32 → int16 (scaled)\n",
        "    img_stack = np.nan_to_num(img_stack, nan=-9999)\n",
        "    img_stack = (img_stack * config[\"save_scale\"]).astype(config[\"save_dtype\"])\n",
        "\n",
        "    # Save image\n",
        "    with rasterio.open(\n",
        "        img_dir / f\"{chip_id}.tif\", \"w\",\n",
        "        driver=\"GTiff\",\n",
        "        height=vv.shape[0], width=vv.shape[1],\n",
        "        count=2,\n",
        "        dtype=config[\"save_dtype\"],\n",
        "        compress=config[\"compression\"]\n",
        "    ) as dst:\n",
        "        dst.write(img_stack)\n",
        "\n",
        "    # Save mask (uint8, 0/1)\n",
        "    mask_binary = (mask > 0.5).astype(np.uint8)\n",
        "    with rasterio.open(\n",
        "        mask_dir / f\"{chip_id}.tif\", \"w\",\n",
        "        driver=\"GTiff\",\n",
        "        height=mask.shape[0], width=mask.shape[1],\n",
        "        count=1,\n",
        "        dtype=np.uint8,\n",
        "        compress=config[\"compression\"]\n",
        "    ) as dst:\n",
        "        dst.write(mask_binary, 1)\n",
        "\n",
        "\n",
        "def generate_dataset(scenes, labels_df, config=CONFIG):\n",
        "    \"\"\"Process all scenes and save chips\"\"\"\n",
        "\n",
        "    output_dir = config[\"output_dir\"]\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Limit scenes for POC\n",
        "    if \"max_scenes\" in config:\n",
        "        scenes = scenes[:config[\"max_scenes\"]]\n",
        "        print(f\"Limited to {len(scenes)} scenes (POC mode)\")\n",
        "\n",
        "    all_chips = []\n",
        "    stats = {\"positive\": 0, \"negative\": 0, \"errors\": 0}\n",
        "\n",
        "    for scene_id in tqdm(scenes, desc=\"Processing scenes\"):\n",
        "        try:\n",
        "            chips = process_scene(scene_id, labels_df, config)\n",
        "\n",
        "            for chip_id, vv, vh, mask, is_positive in chips:\n",
        "                save_chip(chip_id, vv, vh, mask, output_dir, config)\n",
        "                all_chips.append((chip_id, is_positive))\n",
        "\n",
        "                if is_positive:\n",
        "                    stats[\"positive\"] += 1\n",
        "                else:\n",
        "                    stats[\"negative\"] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {scene_id}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            stats[\"errors\"] += 1\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Dataset Generation Complete!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Positive chips: {stats['positive']:,}\")\n",
        "    print(f\"Negative chips: {stats['negative']:,}\")\n",
        "    print(f\"Total: {stats['positive'] + stats['negative']:,}\")\n",
        "    print(f\"Errors: {stats['errors']}\")\n",
        "    print(f\"Output: {output_dir}\")\n",
        "\n",
        "    return all_chips, stats"
      ],
      "metadata": {
        "id": "nUZLE-KToOGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# GENERATE TRAIN/VAL/TEST SPLITS\n",
        "# ============================================================\n",
        "\n",
        "def create_splits(all_chips, train_scenes, val_scenes, config=CONFIG):\n",
        "    \"\"\"Create train/val/test split files\"\"\"\n",
        "\n",
        "    output_dir = config[\"output_dir\"]\n",
        "    splits_dir = output_dir / \"splits\"\n",
        "    splits_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    train_chips = []\n",
        "    test_chips = []\n",
        "    val_chips = []\n",
        "\n",
        "    for chip_id, is_positive in all_chips:\n",
        "        scene_id = \"_\".join(chip_id.split(\"_\")[:-2])  # Extract scene from chip_id\n",
        "        if chip_id.endswith(\"_neg\"):\n",
        "            scene_id = \"_\".join(chip_id.split(\"_\")[:-3])\n",
        "\n",
        "        if scene_id in val_scenes:\n",
        "            val_chips.append(chip_id)\n",
        "        elif scene_id in train_scenes:\n",
        "            # Split train into train + test\n",
        "            if np.random.random() < config[\"test_fraction\"]:\n",
        "                test_chips.append(chip_id)\n",
        "            else:\n",
        "                train_chips.append(chip_id)\n",
        "\n",
        "    # Shuffle\n",
        "    np.random.shuffle(train_chips)\n",
        "    np.random.shuffle(test_chips)\n",
        "    np.random.shuffle(val_chips)\n",
        "\n",
        "    # Save\n",
        "    for name, chips in [(\"train\", train_chips), (\"val\", val_chips), (\"test\", test_chips)]:\n",
        "        with open(splits_dir / f\"{name}.txt\", \"w\") as f:\n",
        "            f.write(\"\\n\".join(chips))\n",
        "        print(f\"{name}: {len(chips):,} chips\")\n",
        "\n",
        "    return train_chips, val_chips, test_chips"
      ],
      "metadata": {
        "id": "VI9bsco6oS4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXECUTE DATASET GENERATION\n",
        "# ============================================================\n",
        "\n",
        "# 1. Filter labels\n",
        "if CONFIG[\"only_confirmed_vessels\"]:\n",
        "    filtered_train = train_df[train_df[\"is_vessel\"] == 1].copy()\n",
        "    filtered_val = val_df[val_df[\"is_vessel\"] == 1].copy()\n",
        "else:\n",
        "    filtered_train = train_df.copy()\n",
        "    filtered_val = val_df.copy()\n",
        "\n",
        "print(f\"Filtered labels: {len(filtered_train):,} train, {len(filtered_val):,} val\")\n",
        "\n",
        "# 2. Get available scenes\n",
        "train_scene_ids = set(filtered_train[\"scene_id\"].unique()) & set(scenes)\n",
        "val_scene_ids = set(filtered_val[\"scene_id\"].unique()) & set(scenes)\n",
        "\n",
        "print(f\"Available scenes: {len(train_scene_ids)} train, {len(val_scene_ids)} val\")\n",
        "\n",
        "# 3. Combine labels\n",
        "all_labels = pd.concat([filtered_train, filtered_val], ignore_index=True)\n",
        "all_scene_ids = list(train_scene_ids | val_scene_ids)\n",
        "\n",
        "print(f\"Processing {len(all_scene_ids)} scenes...\")\n",
        "\n",
        "# 4. Generate!\n",
        "all_chips, stats = generate_dataset(all_scene_ids, all_labels, CONFIG)\n",
        "\n",
        "# 5. Create splits\n",
        "train_chips, val_chips, test_chips = create_splits(\n",
        "    all_chips,\n",
        "    train_scene_ids,\n",
        "    val_scene_ids,\n",
        "    CONFIG\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Done! Dataset saved to {CONFIG['output_dir']}\")"
      ],
      "metadata": {
        "id": "D-4rlhQ4oW3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# VERIFY DATASET\n",
        "# ============================================================\n",
        "\n",
        "def verify_dataset(config=CONFIG, n_samples=6):\n",
        "    \"\"\"Visual verification of generated chips\"\"\"\n",
        "    img_dir = config[\"output_dir\"] / \"images\"\n",
        "    mask_dir = config[\"output_dir\"] / \"masks\"\n",
        "\n",
        "    # Get random samples\n",
        "    all_chips = list(img_dir.glob(\"*.tif\"))\n",
        "    samples = np.random.choice(all_chips, min(n_samples, len(all_chips)), replace=False)\n",
        "\n",
        "    fig, axes = plt.subplots(n_samples, 3, figsize=(12, 4 * n_samples))\n",
        "\n",
        "    for idx, img_path in enumerate(samples):\n",
        "        chip_id = img_path.stem\n",
        "        mask_path = mask_dir / f\"{chip_id}.tif\"\n",
        "\n",
        "        with rasterio.open(img_path) as src:\n",
        "            vv = src.read(1)\n",
        "            vh = src.read(2)\n",
        "\n",
        "        with rasterio.open(mask_path) as src:\n",
        "            mask = src.read(1)\n",
        "\n",
        "        rgb = sar_to_rgb(vv, vh)\n",
        "\n",
        "        axes[idx, 0].imshow(rgb)\n",
        "        axes[idx, 0].set_title(f\"RGB: {chip_id[:20]}...\")\n",
        "        axes[idx, 0].axis(\"off\")\n",
        "\n",
        "        axes[idx, 1].imshow(mask, cmap=\"Reds\", vmin=0, vmax=1)\n",
        "        axes[idx, 1].set_title(f\"Mask: {mask.sum()} ship pixels\")\n",
        "        axes[idx, 1].axis(\"off\")\n",
        "\n",
        "        axes[idx, 2].imshow(rgb)\n",
        "        axes[idx, 2].imshow(mask, cmap=\"Reds\", alpha=0.5)\n",
        "        axes[idx, 2].set_title(\"Overlay\")\n",
        "        axes[idx, 2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Stats\n",
        "    print(f\"\\nDataset Stats:\")\n",
        "    print(f\"  Images: {len(list(img_dir.glob('*.tif'))):,}\")\n",
        "    print(f\"  Masks: {len(list(mask_dir.glob('*.tif'))):,}\")\n",
        "\n",
        "verify_dataset()\n"
      ],
      "metadata": {
        "id": "BO2u6lgpopdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "yb5IEmpaN_kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "ILDqWD9M1ajp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TRAINING CONFIG\n",
        "# ============================================================\n",
        "\n",
        "TRAIN_CONFIG = {\n",
        "    # Paths (Google Drive - persistent!)\n",
        "    \"data_root\": Path(\"/content/drive/MyDrive/xview3/chips\"),\n",
        "    \"output_dir\": Path(\"/content/drive/MyDrive/xview3/runs\"),\n",
        "\n",
        "    # Model\n",
        "    \"backbone\": \"terramind_v1_small\",  # small für POC, base für Production\n",
        "    \"pretrained\": True,\n",
        "    \"freeze_backbone\": True,  # Schneller, weniger Overfitting bei kleinen Daten\n",
        "\n",
        "    # Training\n",
        "    \"batch_size\": 8,\n",
        "    \"max_epochs\": 30,\n",
        "    \"lr\": 2e-4,  # Höher wenn backbone frozen\n",
        "    \"num_workers\": 2,\n",
        "\n",
        "    # Data\n",
        "    \"num_classes\": 2,  # Background + Ship\n",
        "    \"in_channels\": 2,  # VV + VH\n",
        "    \"chip_size\": 256,\n",
        "    \"save_scale\": 10,  # Wie beim Speichern (dB * 10)\n",
        "}\n",
        "\n",
        "# Verify paths exist\n",
        "assert TRAIN_CONFIG[\"data_root\"].exists(), f\"Data not found: {TRAIN_CONFIG['data_root']}\"\n",
        "print(f\"✓ Data found at {TRAIN_CONFIG['data_root']}\")\n",
        "print(f\"  Images: {len(list((TRAIN_CONFIG['data_root'] / 'images').glob('*.tif')))}\")\n",
        "print(f\"  Masks: {len(list((TRAIN_CONFIG['data_root'] / 'masks').glob('*.tif')))}\")"
      ],
      "metadata": {
        "id": "opD4xrJqOFLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DATA MODULE\n",
        "# ============================================================\n",
        "\n",
        "# Standardization values für Sentinel-1 (dB * save_scale)\n",
        "# Typische SAR dB Werte: VV ~ -15, VH ~ -22\n",
        "# Mit scale=10: VV ~ -150, VH ~ -220\n",
        "S1_MEANS = [-150, -220]  # Angepasst für deine Skalierung\n",
        "S1_STDS = [50, 50]       # Rough estimate\n",
        "\n",
        "# Augmentations\n",
        "train_transform = albumentations.Compose([\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "    albumentations.VerticalFlip(p=0.5),\n",
        "    albumentations.RandomRotate90(p=0.5),\n",
        "    albumentations.pytorch.transforms.ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = albumentations.Compose([\n",
        "    albumentations.pytorch.transforms.ToTensorV2(),\n",
        "])\n",
        "\n",
        "# DataModule\n",
        "datamodule = GenericNonGeoSegmentationDataModule(\n",
        "    batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
        "    num_workers=TRAIN_CONFIG[\"num_workers\"],\n",
        "    num_classes=TRAIN_CONFIG[\"num_classes\"],\n",
        "\n",
        "    # Data paths (all from Google Drive)\n",
        "    train_data_root=str(TRAIN_CONFIG[\"data_root\"] / \"images\"),\n",
        "    train_label_data_root=str(TRAIN_CONFIG[\"data_root\"] / \"masks\"),\n",
        "    val_data_root=str(TRAIN_CONFIG[\"data_root\"] / \"images\"),\n",
        "    val_label_data_root=str(TRAIN_CONFIG[\"data_root\"] / \"masks\"),\n",
        "    test_data_root=str(TRAIN_CONFIG[\"data_root\"] / \"images\"),\n",
        "    test_label_data_root=str(TRAIN_CONFIG[\"data_root\"] / \"masks\"),\n",
        "\n",
        "    # Split files\n",
        "    train_split=str(TRAIN_CONFIG[\"data_root\"] / \"splits\" / \"train.txt\"),\n",
        "    val_split=str(TRAIN_CONFIG[\"data_root\"] / \"splits\" / \"val.txt\"),\n",
        "    test_split=str(TRAIN_CONFIG[\"data_root\"] / \"splits\" / \"test.txt\"),\n",
        "\n",
        "    # File patterns\n",
        "    img_grep=\"*.tif\",\n",
        "    label_grep=\"*.tif\",\n",
        "\n",
        "    # Normalization\n",
        "    means=S1_MEANS,\n",
        "    stds=S1_STDS,\n",
        "\n",
        "    # Transforms\n",
        "    train_transform=train_transform,\n",
        "    val_transform=val_transform,\n",
        "    test_transform=val_transform,\n",
        "\n",
        "    # NaN handling\n",
        "    no_label_replace=-1,\n",
        "    no_data_replace=0,\n",
        ")\n",
        "\n",
        "# Setup\n",
        "datamodule.setup(\"fit\")\n",
        "print(f\"✓ Train samples: {len(datamodule.train_dataset)}\")\n",
        "print(f\"✓ Val samples: {len(datamodule.val_dataset)}\")"
      ],
      "metadata": {
        "id": "SUXZhRHQOZWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# VERIFY DATA LOADING\n",
        "# ============================================================\n",
        "\n",
        "# Check one batch\n",
        "batch = next(iter(datamodule.train_dataloader()))\n",
        "images = batch[\"image\"]\n",
        "masks = batch[\"mask\"]\n",
        "\n",
        "print(f\"Image batch shape: {images.shape}\")  # [B, 2, 256, 256]\n",
        "print(f\"Mask batch shape: {masks.shape}\")    # [B, 256, 256]\n",
        "print(f\"Image range: [{images.min():.2f}, {images.max():.2f}]\")\n",
        "print(f\"Mask unique values: {torch.unique(masks)}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "for i in range(4):\n",
        "    # VV channel\n",
        "    axes[0, i].imshow(images[i, 0].numpy(), cmap=\"gray\")\n",
        "    axes[0, i].set_title(f\"VV {i}\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    # Mask overlay\n",
        "    axes[1, i].imshow(images[i, 0].numpy(), cmap=\"gray\")\n",
        "    axes[1, i].imshow(masks[i].numpy(), cmap=\"Reds\", alpha=0.5)\n",
        "    axes[1, i].set_title(f\"Mask {i}\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Hp_9qMCOchc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MODEL SETUP\n",
        "# ============================================================\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Checkpoint callback\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=str(TRAIN_CONFIG[\"output_dir\"] / \"checkpoints\"),\n",
        "    monitor=\"val/mIoU\",\n",
        "    mode=\"max\",\n",
        "    filename=\"best-{epoch:02d}-{val/mIoU:.4f}\",\n",
        "    save_top_k=1,\n",
        "    save_weights_only=True,\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stop = pl.callbacks.EarlyStopping(\n",
        "    monitor=\"val/mIoU\",\n",
        "    patience=10,\n",
        "    mode=\"max\",\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = terratorch.tasks.SemanticSegmentationTask(\n",
        "    model_factory=\"EncoderDecoderFactory\",\n",
        "    model_args={\n",
        "        # TerraMind backbone\n",
        "        \"backbone\": TRAIN_CONFIG[\"backbone\"],\n",
        "        \"backbone_pretrained\": TRAIN_CONFIG[\"pretrained\"],\n",
        "\n",
        "        # SAR input - 2 Kanäle (VV, VH)\n",
        "        # Option A: Neuen Patch Embedding erstellen\n",
        "        \"backbone_modalities\": [],\n",
        "        \"backbone_in_chans\": TRAIN_CONFIG[\"in_channels\"],\n",
        "\n",
        "        # Necks für hierarchischen Decoder\n",
        "        \"necks\": [\n",
        "            {\"name\": \"SelectIndices\", \"indices\": [2, 5, 8, 11]},  # small/base\n",
        "            {\"name\": \"ReshapeTokensToImage\", \"remove_cls_token\": False},\n",
        "            {\"name\": \"LearnedInterpolateToPyramidal\"},\n",
        "        ],\n",
        "\n",
        "        # Decoder\n",
        "        \"decoder\": \"UNetDecoder\",\n",
        "        \"decoder_channels\": [256, 128, 64, 32],\n",
        "\n",
        "        # Head\n",
        "        \"head_dropout\": 0.1,\n",
        "        \"num_classes\": TRAIN_CONFIG[\"num_classes\"],\n",
        "    },\n",
        "\n",
        "    # Training config\n",
        "    loss=\"dice\",  # Gut für unbalanced segmentation\n",
        "    optimizer=\"AdamW\",\n",
        "    lr=TRAIN_CONFIG[\"lr\"],\n",
        "    ignore_index=-1,\n",
        "    freeze_backbone=TRAIN_CONFIG[\"freeze_backbone\"],\n",
        "    freeze_decoder=False,\n",
        "\n",
        "    # Logging\n",
        "    plot_on_val=True,\n",
        "    class_names=[\"Background\", \"Ship\"],\n",
        ")\n",
        "\n",
        "print(f\"✓ Model created: {TRAIN_CONFIG['backbone']}\")\n",
        "print(f\"  Backbone frozen: {TRAIN_CONFIG['freeze_backbone']}\")"
      ],
      "metadata": {
        "id": "YBhb1rtVOfN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TRAINING\n",
        "# ============================================================\n",
        "\n",
        "# Trainer\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    precision=\"16-mixed\",  # Faster training\n",
        "    max_epochs=TRAIN_CONFIG[\"max_epochs\"],\n",
        "    logger=pl.loggers.TensorBoardLogger(\n",
        "        save_dir=str(TRAIN_CONFIG[\"output_dir\"]),\n",
        "        name=\"tensorboard\",\n",
        "    ),\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        early_stop,\n",
        "        pl.callbacks.RichProgressBar(),\n",
        "    ],\n",
        "    log_every_n_steps=5,\n",
        "    default_root_dir=str(TRAIN_CONFIG[\"output_dir\"]),\n",
        ")\n",
        "\n",
        "# Optional: TensorBoard starten\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {TRAIN_CONFIG[\"output_dir\"]}\n",
        "\n",
        "# TRAIN!\n",
        "trainer.fit(model, datamodule=datamodule)\n",
        "\n",
        "print(f\"\\n✅ Training complete!\")\n",
        "print(f\"Best checkpoint: {checkpoint_callback.best_model_path}\")"
      ],
      "metadata": {
        "id": "cnUrjoC6OjNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "# Test on best checkpoint\n",
        "datamodule.setup(\"test\")\n",
        "results = trainer.test(\n",
        "    model,\n",
        "    datamodule=datamodule,\n",
        "    ckpt_path=checkpoint_callback.best_model_path\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "for k, v in results[0].items():\n",
        "    print(f\"  {k}: {v:.4f}\")"
      ],
      "metadata": {
        "id": "TY-JCrvmOmYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INFERENCE VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "# Load best model\n",
        "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
        "    checkpoint_callback.best_model_path,\n",
        "    model_factory=\"EncoderDecoderFactory\",\n",
        "    model_args=model.hparams.model_args,\n",
        ")\n",
        "model.eval()\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Predict on test samples\n",
        "test_loader = datamodule.test_dataloader()\n",
        "batch = next(iter(test_loader))\n",
        "images = batch[\"image\"].to(model.device)\n",
        "masks_gt = batch[\"mask\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "    preds = torch.argmax(outputs.output, dim=1).cpu()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
        "for i in range(4):\n",
        "    # Input\n",
        "    axes[i, 0].imshow(images[i, 0].cpu(), cmap=\"gray\")\n",
        "    axes[i, 0].set_title(\"Input (VV)\")\n",
        "    axes[i, 0].axis(\"off\")\n",
        "\n",
        "    # Ground Truth\n",
        "    axes[i, 1].imshow(masks_gt[i], cmap=\"Reds\")\n",
        "    axes[i, 1].set_title(f\"GT ({masks_gt[i].sum()} px)\")\n",
        "    axes[i, 1].axis(\"off\")\n",
        "\n",
        "    # Prediction\n",
        "    axes[i, 2].imshow(preds[i], cmap=\"Reds\")\n",
        "    axes[i, 2].set_title(f\"Pred ({preds[i].sum()} px)\")\n",
        "    axes[i, 2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JEpBe3-mOp2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1KW6lW_OrHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}